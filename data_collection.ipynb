{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "The purpose of this analysis is to examine the impact of the amplification of COVID-19 related discussions within Twitter on financial markets.\n",
    "\n",
    "## COVID-19 Tweets & Setiment Scores\n",
    "\n",
    "**Why use twitter?**\n",
    "\n",
    "**About the dataset**\n",
    "\n",
    "Rabindra Lamsai of the School of Computer and Systems Sciences - Jawaharlal Nehru University (New Delhi) provides a [dataset](https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset) that is an excellent fit for the puroses of this analysis. The data provided is a set of csv files that contain tweets related to the COVID-19 pandemic and their associated sentiment scores.\n",
    "\n",
    "Theses tweets were collected as a part of an [ongoing project](https://live.rlamsal.com.np). It monitors Twitter in real time for COVID-19 related tweets by filtering by 54 different keywords that are commonly sued while referencing the pandemic (insert citation).\n",
    "\n",
    "**Twitter content redistribution policy**\n",
    "\n",
    "The CSVs in this dataset do not actually contian tweet data, but rather tweet IDs. This was done in order to comply with Twitter's content redistribution policy. Tweet IDs in the dataset will need to be hydrated before analsysis is possible.\n",
    "\n",
    "### Web Scraping\n",
    "For this analysis, we will require third-party data. While this requires some level of manual retrieval, we will automate the data collection process as much as possible to esnure the reproducability of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing site structure\n",
    "\n",
    "Now that we have successfully imported BeautifulSoup and created an object that contains the html of our desired webpage, we must analyze the site's structure to generate a methodology for scraping the desired data.\n",
    "\n",
    "Here is a quick scroll through of the [page](https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset) that we are interested in:\n",
    "\n",
    "<img src=\"./images/iee-webpage.gif\">\n",
    "\n",
    "All of the data that we need to retireve is located within the right sidebar, however there are well over 100 links that need to be clicked to access the data. Furthermore, these files will need to be moved to the desired directory and subsequently concatenated. To make this process more efficient, automation should be implemented. To do so we will need to employ webscraping techniques in both selenium and BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating data collection\n",
    "\n",
    "The dataset files are all located in a sidebar on the right of the page. We will do the following to programatically retrieve all 132 of these files. The firs step in doing so is selecting the appropriate page elements and adding them to BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging in with selenium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import secrets # create a secrets.py file in the root of the project directory and enter confiential info there\n",
    "\n",
    "url = 'https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset'\n",
    "\n",
    "ieee_username = secrets.ieee_username\n",
    "ieee_password = secrets.ieee_password\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "login_link = driver.find_element_by_xpath('//*[@id=\"login\"]/a[2]')\n",
    "login_link.click()\n",
    "\n",
    "username_input = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[3]/div/div/form/div/div/div/div[2]/div[1]/input[1]')\n",
    "username_input.send_keys(ieee_username)\n",
    "\n",
    "password_input = driver.find_element_by_xpath('//*[@id=\"password\"]')\n",
    "password_input.send_keys(ieee_password)\n",
    "\n",
    "submit_login = driver.find_element_by_xpath('//*[@id=\"modalWindowRegisterSignInBtn\"]')\n",
    "submit_login.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Passing the webpage to BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import os\n",
    "\n",
    "soup = bs(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Retrieving the download links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store all of the links that we scrape from the webpage\n",
    "data_files = []\n",
    "\n",
    "# Loop through links that contain a string which matches the regex defined below\n",
    "# Regex: match text that contains corona_tweets_, followed by one or more digits, a period and either csv or zip\n",
    "for link in soup.find_all('a', string = re.compile('corona_tweets_\\d+\\.(csv|zip)')):\n",
    "    \n",
    "    # For each a tag (i.e. link), pull out the value of its href attribute.     \n",
    "    download_link = link['href']\n",
    "    # Append each link to the data_files list defined above\n",
    "    data_files.append(download_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Downloading csv and zip files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Loop through urls in our array\n",
    "# Search each url using regex to look for the name of the desired file.\n",
    "for url in data_files:\n",
    "    \n",
    "    # If the url has a .csv extension...\n",
    "    if re.search(r'corona_tweets_\\d+\\.csv', url):\n",
    "        \n",
    "        # create a string from the regex search that will be used as the file name...\n",
    "        csv_file_name = re.search(r'corona_tweets_\\d+\\.csv', url).group(0)\n",
    "        \n",
    "        # create a variable to store our full path by joining our desired save location with the csv file name...\n",
    "        # Note: downloaded file will be saved to /covid-tweets/raw_data/csv within our project directory\n",
    "        csv_full_file_name = os.path.join('./covid-tweets/raw_data/csv', csv_file_name)\n",
    "        \n",
    "        # and finally use urllib to retrieve the file\n",
    "        urllib.request.urlretrieve(url, csv_full_file_name)\n",
    "        \n",
    "    # When the file does not match the regex search above...\n",
    "    else:\n",
    "        \n",
    "        # create a string from the regex search that will be used as the file name...\n",
    "        zip_file_name = re.search(r'corona_tweets_\\d+\\.zip', url).group(0)\n",
    "        \n",
    "        # create a variable to store our full path by joining our desired save location with the zip file name...\n",
    "        # Note: downloaded file will be saved to /covid-tweets/raw_data/zip within our project directory\n",
    "        zip_full_file_name = os.path.join('./covid-tweets/raw_data/zip', zip_file_name)\n",
    "        urllib.request.urlretrieve(url, zip_full_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add twark to the project and use it to hydrate the tweet IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Prior to analyzing our data, it is important to first ensure data integrity by properly cleaning our desired data sets. Doing so will include several steps as several data sets from multiple third party sources must be used in the abscence of satisfactory first party data. In order to get to a state where data cleaning is feasible, we will need to simplify our files.\n",
    "\n",
    "Creating a concise set of data that can be cleaned appropriately requires the following steps:\n",
    "1. Extracting the zip files\n",
    "2. Concatenating all raw data csv files\n",
    "3. Hydrating the tweets within the csv files\n",
    "4. Cleaning the resulting tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating COVID-19 Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# Change working directory to raw_data where we all of our raw data is stored. Can comment out once directory has changed.\n",
    "# os.chdir(\"./covid-tweets/raw_data\")\n",
    "\n",
    "# create a variable to hold our desired extension, csv in this case.\n",
    "# extension = 'csv'\n",
    "\n",
    "# store all filenames in current working directory that have a csv extension.\n",
    "# all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "# concatenate all of our files stored in all_filenames using pandas and save output to a new variable.\n",
    "# combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "\n",
    "# export our combined_csv as a csv file to a separate folder named raw_data_concat for ease of access.\n",
    "# combined_csv.to_csv(\"../raw_data_concat/raw_data_concat.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Change back to root directory of project to allow for the continuous running of the script\n",
    "# os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check for the current working directory at any time to resolve issues related to save locations, use the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Hydrating tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add twark to the project and use it to hydrate the tweet IDs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
