{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Between COVID-19 Twitter Mentions and Market Indices\n",
    "\n",
    "The purpose of this analysis is to examine the impact of the amplification of COVID-19 related discussions within Twitter on financial markets.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "### Web Scraping\n",
    "For this analysis, we will require third-party data. While this requires some level of manual retrieval, we will automate the data collection process as much as possible to esnure the reproducability of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging in with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import secrets\n",
    "import time\n",
    "\n",
    "url = 'https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset'\n",
    "\n",
    "ieee_username = secrets.ieee_username\n",
    "ieee_password = secrets.ieee_password\n",
    "\n",
    "def wait(x): \n",
    "    time.sleep(x)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "wait(2)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "wait(2)\n",
    "\n",
    "login_link = driver.find_element_by_xpath('//*[@id=\"login\"]/a[2]')\n",
    "login_link.click()\n",
    "\n",
    "wait(2)\n",
    "\n",
    "username_input = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[3]/div/div/form/div/div/div/div[2]/div[1]/input[1]')\n",
    "username_input.send_keys(ieee_username)\n",
    "\n",
    "wait(2)\n",
    "\n",
    "password_input = driver.find_element_by_xpath('//*[@id=\"password\"]')\n",
    "password_input.send_keys(ieee_password)\n",
    "\n",
    "wait(2)\n",
    "\n",
    "submit_login = driver.find_element_by_xpath('//*[@id=\"modalWindowRegisterSignInBtn\"]')\n",
    "submit_login.click()\n",
    "\n",
    "# Logout of website so that we can re-run code without any issues\n",
    "# logout_link = driver.find_element_by_xpath('//*[@id=\"login\"]/a[3]')\n",
    "# logout_link.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\n",
      "  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html dir=\"ltr\" version=\"XHTML+RDFa 1.0\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:article=\"http://ogp.me/ns/article#\" xmlns:book=\"http://ogp.me/ns/book#\" xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:dc=\"http://purl.org/dc/terms/\" xmlns:foaf=\"http://xmlns.com/foaf/0.1/\" xmlns:og=\"http://ogp.me/ns#\" xmlns:product=\"http://ogp.me/ns/product#\" xmlns:profile=\"http\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup as bs\n",
    "# from requests import get\n",
    "\n",
    "# # Create variable to store the URL from which we plan to pull COVID-19 tweet data\n",
    "# url = 'https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset'\n",
    "\n",
    "# # Use get to retrieve webpage HTML and store in response variable\n",
    "# response = get(url)\n",
    "\n",
    "# # Parse response.text with 'htlm.parser' and store it in a variable\n",
    "# html = bs(response.text, 'html.parser')\n",
    "\n",
    "# # Check that our object html contains the html content of the page that we will scrape\n",
    "# print(html.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing site structure\n",
    "\n",
    "Now that we have successfully imported BeautifulSoup and created an object that contains the html of our desired webpage, we must analyze the site's structure to generate a methodology for scraping the desired data.\n",
    "\n",
    "Here is a quick scroll through of the page that we are interested in:\n",
    "\n",
    "<img src=\"./images/iee-scroll-through.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pulling Dataset Files**\n",
    "\n",
    "The dataset files are all located in a sidebar on the right of the page. We will do the following to programatically retrieve all 132 of these files. The firs step in doing so is selecting the appropriate page elements and adding them to BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right_column = html.find('div', id ='right-column')\n",
    "\n",
    "# print(right_column.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Prior to analyzing our data, it is important to first ensure data integrity by properly cleaning our desired data sets. Doing so will include several steps as several data sets from multiple third party sources must be used in the abscence of satisfactory first party data. In order to get to a state where data cleaning is feasible, we will need to simplify our files.\n",
    "\n",
    "Creating a concise set of data that can be cleaned appropriately requires the following steps:\n",
    "1. Extracting the zip files\n",
    "2. Concatenating the csv files\n",
    "3. Hydrating the tweets within the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating COVID-19 Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xaviergill/Documents/coding-projects/covid_tweet_and_market_analysis/covid-tweets/raw_data'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Change working directory to raw_data where we all of our raw data is stored. Can comment out once directory has changed.\n",
    "# os.chdir(\"./covid-tweets/raw_data\")\n",
    "\n",
    "# create a variable to hold our desired extension, csv in this case.\n",
    "extension = 'csv'\n",
    "\n",
    "# store all filenames in current working directory that have a csv extension.\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "# concatenate all of our files stored in all_filenames using pandas and save output to a new variable.\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "\n",
    "# export our combined_csv as a csv file to a separate folder named raw_data_concat for ease of access.\n",
    "combined_csv.to_csv(\"../raw_data_concat/raw_data_concat.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Change back to root directory of project to allow for the continuous running of the script\n",
    "# os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
