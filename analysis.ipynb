{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Between COVID-19 Twitter Mentions and Market Indices\n",
    "\n",
    "The purpose of this analysis is to examine the impact of the amplification of COVID-19 related discussions within Twitter on financial markets.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "### Web Scraping\n",
    "For this analysis, we will require third-party data. While this requires some level of manual retrieval, we will automate the data collection process as much as possible to esnure the reproducability of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging in with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import secrets\n",
    "import time\n",
    "\n",
    "url = 'https://ieee-dataport.org/open-access/coronavirus-covid-19-tweets-dataset'\n",
    "\n",
    "ieee_username = secrets.ieee_username\n",
    "ieee_password = secrets.ieee_password\n",
    "\n",
    "# def wait(x): \n",
    "#     time.sleep(x)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# wait(2)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# wait(2)\n",
    "\n",
    "login_link = driver.find_element_by_xpath('//*[@id=\"login\"]/a[2]')\n",
    "login_link.click()\n",
    "\n",
    "# wait(2)\n",
    "\n",
    "username_input = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[3]/div/div/form/div/div/div/div[2]/div[1]/input[1]')\n",
    "username_input.send_keys(ieee_username)\n",
    "\n",
    "# wait(2)\n",
    "\n",
    "password_input = driver.find_element_by_xpath('//*[@id=\"password\"]')\n",
    "password_input.send_keys(ieee_password)\n",
    "\n",
    "# wait(2)\n",
    "\n",
    "submit_login = driver.find_element_by_xpath('//*[@id=\"modalWindowRegisterSignInBtn\"]')\n",
    "submit_login.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the webpage to BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://ieee-dataport.s3.amazonaws.com/open/14206/corona_tweets_01.csv?response-content-disposition=attachment%3B%20filename%3D%22corona_tweets_01.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20200801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200801T211320Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=5d9f3a0126ee5a1ed4cce3702bab858abd8fb62c5c15952f2c8c3567f6fd0028', 'https://ieee-dataport.s3.amazonaws.com/open/14206/corona_tweets_02.csv?response-content-disposition=attachment%3B%20filename%3D%22corona_tweets_02.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20200801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200801T211320Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=50163f772439c813da48c8fcceb2e25f57848e5a5a4b78a8b2cfc1ee41731a30', 'https://ieee-dataport.s3.amazonaws.com/open/14206/corona_tweets_03.csv?response-content-disposition=attachment%3B%20filename%3D%22corona_tweets_03.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20200801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200801T211320Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=61598d7279a2def7c8ec8883ef9b3ba78ff823e7fce6d3e589dc4db983d19777', 'https://ieee-dataport.s3.amazonaws.com/open/14206/corona_tweets_04.csv?response-content-disposition=attachment%3B%20filename%3D%22corona_tweets_04.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20200801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200801T211320Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=05b287e828388dedce9c417669a6e83fe231f054225d40c0875f3a8c0a493018', 'https://ieee-dataport.s3.amazonaws.com/open/14206/corona_tweets_05.csv?response-content-disposition=attachment%3B%20filename%3D%22corona_tweets_05.csv%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20200801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200801T211320Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=13c588824e181ebddc669c42e603596cf893e1368a8914f0a46aefb0d651fe50']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "soup = bs(driver.page_source)\n",
    "\n",
    "# Create empty list to store all of the links that we scrape from the webpage\n",
    "data_files = []\n",
    "\n",
    "# Loop through links that contain a string which matches the regex defined below\n",
    "# Regex: match text that contains corona_tweets_, followed by one or more digits, a period and either csv or zip\n",
    "for link in soup.find_all('a', string = re.compile('corona_tweets_\\d+\\.(csv|zip)')):\n",
    "    \n",
    "    # For each a tag (i.e. link), pull out the value of its href attribute.     \n",
    "    download_link = link['href']\n",
    "    # Append each link to the data_files list defined above\n",
    "    data_files.append(download_link)\n",
    "    \n",
    "    \n",
    "print(data_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "for link in data_files:\n",
    "    url = link\n",
    "    file_name = re.search(r'corona_tweets_\\d+\\.(csv|zip)', link).group(0)\n",
    "    full_file_name = os.path.join('./covid-tweets/raw_data', file_name)\n",
    "    urllib.request.urlretrieve(url, full_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing site structure\n",
    "\n",
    "Now that we have successfully imported BeautifulSoup and created an object that contains the html of our desired webpage, we must analyze the site's structure to generate a methodology for scraping the desired data.\n",
    "\n",
    "Here is a quick scroll through of the page that we are interested in:\n",
    "\n",
    "<img src=\"./images/iee-scroll-through.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pulling Dataset Files**\n",
    "\n",
    "The dataset files are all located in a sidebar on the right of the page. We will do the following to programatically retrieve all 132 of these files. The firs step in doing so is selecting the appropriate page elements and adding them to BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right_column = html.find('div', id ='right-column')\n",
    "\n",
    "# print(right_column.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Prior to analyzing our data, it is important to first ensure data integrity by properly cleaning our desired data sets. Doing so will include several steps as several data sets from multiple third party sources must be used in the abscence of satisfactory first party data. In order to get to a state where data cleaning is feasible, we will need to simplify our files.\n",
    "\n",
    "Creating a concise set of data that can be cleaned appropriately requires the following steps:\n",
    "1. Extracting the zip files\n",
    "2. Concatenating the csv files\n",
    "3. Hydrating the tweets within the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating COVID-19 Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xaviergill/Documents/coding-projects/covid_tweet_and_market_analysis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Change working directory to raw_data where we all of our raw data is stored. Can comment out once directory has changed.\n",
    "# os.chdir(\"./covid-tweets/raw_data\")\n",
    "\n",
    "# create a variable to hold our desired extension, csv in this case.\n",
    "# extension = 'csv'\n",
    "\n",
    "# store all filenames in current working directory that have a csv extension.\n",
    "# all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "# concatenate all of our files stored in all_filenames using pandas and save output to a new variable.\n",
    "# combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "\n",
    "# export our combined_csv as a csv file to a separate folder named raw_data_concat for ease of access.\n",
    "# combined_csv.to_csv(\"../raw_data_concat/raw_data_concat.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Change back to root directory of project to allow for the continuous running of the script\n",
    "# os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xaviergill/Documents/coding-projects/covid_tweet_and_market_analysis'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
